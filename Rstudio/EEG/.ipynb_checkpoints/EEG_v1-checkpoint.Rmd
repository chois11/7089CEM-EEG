---
title: "This notebook is made for as part of Coventry University's 7089CEM cousework"
output: html_notebook
---

The code is written in R language
Made by Sunggu Choi

# Task 1.1
Data analysis

```{r}
# Install libraies
#install.packages("corrplot") # install if required

```

```{r}
# Load required libaraies 
library(plotly)
library(corrplot)
```

```{r}
eeg_x = read.csv("x.csv", header = FALSE)
eeg_y = read.csv("y.csv", header = FALSE)
eeg_time = read.csv("time.csv", header = FALSE)
```

```{r}
eeg_x_y <- cbind(eeg_x,eeg_y)
eeg_combined <- cbind(eeg_time, eeg_x, eeg_y)
```

```{r}
colnames(eeg_combined)[1] <- "time"
colnames(eeg_combined)[6] <- "result"
```

```{r}
par(mfrow=c(2,2))
matplot(eeg_time, eeg_x[1], main="V1", xlab="Time", ylab="Input X", type="l",col=c("black"),lty=c(1,1))
matplot(eeg_time, eeg_x[2], main="V2", xlab="Time",  ylab="Input X", type="l",col=c("black"),lty=c(1,1))
matplot(eeg_time, eeg_x[3], main="V3", xlab="Time",  ylab="Input X", type="l",col=c("black"),lty=c(1,1))
matplot(eeg_time, eeg_x[4], main="V4", xlab="Time",  ylab="Input X", type="l",col=c("black"),lty=c(1,1))
matplot(eeg_time, eeg_y, main="V1 Result", xlab="Time", ylab="Output Y", type="l",col=c("black"),lty=c(1,1))
```

```{r}
matplot(eeg_time, eeg_y, main="V1 Result", xlab="Time", ylab="Output Y", type="l",col=c("black"),lty=c(1,1))
```

## Task 1.2 Distribution graph

```{r}
# Kernel Density Plot
par(mfrow=c(2,2))
eeg_x1 <- density(eeg_x$V1) # returns the density data
eeg_x2 <- density(eeg_x$V2) # returns the density data
eeg_x3 <- density(eeg_x$V3) # returns the density data
eeg_x4 <- density(eeg_x$V4) # returns the density data
plot(eeg_x1 ,main="Distribution V1") # plots the results
plot(eeg_x2 ,main="Distribution V2") # plots the results
plot(eeg_x3 ,main="Distribution V3") # plots the results
plot(eeg_x4, main="Distribution V4") # plots the results
```

## Task 1.3 Correlation and scatter plots

```{r}
x <- eeg_x$V1 # x axis for compare
y <- eeg_y$V1 # y axis for compare 
cor(x, y, method = c("pearson", "kendall", "spearman"))
```

```{r}
# Correlation plot
# par(mfrow=c(2,2))

#X V1
x <- eeg_x$V1 # x axis for compare
y <- eeg_y$V1 # y axis for compare #output
# Plot with main and axis titles

plot(x, y, main = "Correlation between X V1 and Y",
     xlab = "X V1", ylab = "Y V1")
# Add regression line
abline(lm(y ~ x, data = eeg_x[1]), col = "blue")

# #X V2

x <- eeg_x$V2 # x axis for compare
y <- eeg_y$V1 # y axis for compare #output
# Plot with main and axis titles
plot(x, y, main = "Correlation between X V2 and Y",
     xlab = "X V2", ylab = "Y V1")
# Add regression line
#plot(x, y, main = "Correlation between X V2 and Y V1", xlab = "X V2", ylab = "Y V1")
abline(lm(y ~ x, data = eeg_x[2]), col = "blue")


#X V3
x <- eeg_x$V3 # x axis for compare
y <- eeg_y$V1 # y axis for compare #output
# Plot with main and axis titles
plot(x, y, main = "Correlation between X V3 and Y",
     xlab = "X V3", ylab = "Y V1")
# Add regression line
#plot(x, y, main = "Correlation between X V3 and Y V1", xlab = "X V3", ylab = "Y V1")
abline(lm(y ~ x, data = eeg_x[3]), col = "blue")


#X V4
x <- eeg_x$V4 # x axis for compare
y <- eeg_y$V1 # y axis for compare #output
# Plot with main and axis titles
plot(x, y, main = "Correlation between X V4 and Y",
     xlab = "X V4", ylab = "Y V1")
# Add regression line
#plot(x, y, main = "Correlation between X V4 and Y V1", xlab = "X V4", ylab = "Y V1")
abline(lm(y ~ x, data = eeg_x[4]), col = "blue")

```

# Task 2 Estimate model parameters
Identify a true non-linear model

### Task 2.1 Estimate model parameters

```{r}
x1 = eeg_x$V1
x2 = eeg_x$V2
x3 = eeg_x$V3
x4 = eeg_x$V4
y = eeg_y$V1


model_one = cbind(x4, x1^2, x1^3, x3^4, 1)
model_two = cbind(x3^3, x3^4, 1)
model_three = cbind(x2, x1^3, x3^4, 1)
model_four = cbind(x4, x1^3, x3^4, 1)
model_five = cbind(x4, x1^2, x1^3, x3^4,x1^4, 1)

theta_hat_1 <- solve(t(model_one) %*% model_one) %*% t(model_one) %*% y
print("Hat 1")
print(theta_hat_1)

theta_hat_2 <- solve(t(model_two) %*% model_two) %*% t(model_two) %*% y
print("Hat 2")
print(theta_hat_2)

theta_hat_3 <- solve(t(model_three) %*% model_three) %*% t(model_three) %*% y
print("Hat 3")
print(theta_hat_3)

theta_hat_4 <- solve(t(model_four) %*% model_four) %*% t(model_four) %*% y
print("Hat 4")
print(theta_hat_4)

theta_hat_5 <- solve(t(model_five) %*% model_five) %*% t(model_five) %*% y
print("Hat 5")
print(theta_hat_5)

```

### Task 2.2 Compute the model residual (error) sum of squared errors (RSS)

```{r}
model_one_RSS = sum((y - model_one %*% theta_hat_1)^2) # RSS for model one
model_two_RSS = sum((y - model_two %*% theta_hat_2)^2) # RSS for model two
model_three_RSS = sum((y - model_three %*% theta_hat_3)^2) # RSS for model three
model_four_RSS = sum((y - model_four %*% theta_hat_4)^2) # RSS for model four
model_five_RSS = sum((y - model_five %*% theta_hat_5)^2) # RSS for model five

print(model_one_RSS)
print(model_two_RSS)
print(model_three_RSS)
print(model_four_RSS)
print(model_five_RSS)

```

### Task 2.3 Log-likelihood function

```{r}
n1 = nrow(model_one)
sigma_1 = (model_one_RSS/(n1-1))
LLF1 = -(n1/2)*log(2*pi)-(n1/2)*log(sigma_1)-(1/2)/sigma_1*model_one_RSS # log-likelihood function
print(LLF1)

n2 = nrow(model_two)
sigma_2 = (model_two_RSS/(n2-1))
LLF2 = -(n2/2)*log(2*pi)-(n2/2)*log(sigma_2)-(1/2)/sigma_2*model_two_RSS # log-likelihood function
print(LLF2)

n3 = nrow(model_three)
sigma_3 = (model_three_RSS/(n3-1))
LLF3 = -(n3/2)*log(2*pi)-(n3/2)*log(sigma_3)-(1/2)/sigma_3*model_three_RSS # log-likelihood function
print(LLF3)

n4 = nrow(model_four)
sigma_4 = (model_four_RSS/(n4-1))
LLF4 = -(n4/2)*log(2*pi)-(n4/2)*log(sigma_4)-(1/2)/sigma_4*model_four_RSS # log-likelihood function
print(LLF4)

n5 = nrow(model_five)
sigma_5 = (model_five_RSS/(n5-1))
LLF5 = -(n5/2)*log(2*pi)-(n5/2)*log(sigma_5)-(1/2)/sigma_5*model_five_RSS # log-likelihood function
print(LLF5)
```

### Task 2.4 Akaike information criterion (AIC) and Bayesian information criterion (BIC)

```{r}
#AIC = 2k - 2ln p(d|theta_hat)
#BIC = k*ln(n) - 2ln(p(D|theta_hat)) n is number of observations
# (p(D|theta_hat) is LLF
# n is number of row matries 


AIC_1 = 2*length(theta_hat_1) - 2*LLF1 #AIC
AIC_2 = 2*length(theta_hat_2) - 2*LLF2 #AIC
AIC_3 = 2*length(theta_hat_3) - 2*LLF3 #AIC
AIC_4 = 2*length(theta_hat_4) - 2*LLF4 #AIC
AIC_5 = 2*length(theta_hat_5) - 2*LLF5 #AIC

print("########### AIC")
print(AIC_1)
print(AIC_2)
print(AIC_3)
print(AIC_4)
print(AIC_5)

print("############ BIC")
BIC_1 = length(theta_hat_1)*log(n1) - 2*LLF1
BIC_2 = length(theta_hat_2)*log(n2) - 2*LLF2
BIC_3 = length(theta_hat_3)*log(n3) - 2*LLF3
BIC_4 = length(theta_hat_4)*log(n4) - 2*LLF4
BIC_5 = length(theta_hat_5)*log(n5) - 2*LLF5

print(BIC_1)
print(BIC_2)
print(BIC_3)
print(BIC_4)
print(BIC_5)

```

### Task 2.5 Check distribution of model prediction errors with q-q plot

```{r}
error_rate_dis_1 = (y - model_one %*% theta_hat_1) # RSS for model one
error_rate_dis_2 = (y - model_two %*% theta_hat_2) # RSS for model one
error_rate_dis_3 = (y - model_three %*% theta_hat_3) # RSS for model one
error_rate_dis_4 = (y - model_four %*% theta_hat_4) # RSS for model one
error_rate_dis_5 = (y - model_five %*% theta_hat_5) # RSS for model one

#Over-rap between original y and error_rate so can compare.
par(mfrow=c(2,1))
qqnorm(error_rate_dis_1, main="Model one error distribution")
qqline(error_rate_dis_1)

qqnorm(error_rate_dis_2, main="Model two error distribution")
qqline(error_rate_dis_2)

qqnorm(error_rate_dis_3, main="Model three error distribution")
qqline(error_rate_dis_3)

qqnorm(error_rate_dis_4, main="Model four error distribution")
qqline(error_rate_dis_4)

qqnorm(error_rate_dis_5, main="Model five error distribution")
qqline(error_rate_dis_5)


```

### Task 2.6 Select best regression model according to the AIC,BIC and explain
Lowest is best model Model three is lowest

### Task 2.7 Construct machine learning environment

Split the input and output EEG dataset X and y into two parts (70% for training 30% for testing)

1. Estimate model parameter use training dataset
2. Compute the model's output / prediction
3. Compute the 95% (model prediction confidence intervals and plot with error bars and model prediction as well as testing data samples.

### Task 2.7.0 Split the input and output EEG dataset X and y into two parts (70% for training 30% for testing)1. Split the input and output EEG dataset X and y into two parts (70% for training 30% for testing)

```{r}
# Spliting data
# X is EEG_INPUT y is EEG_OUTPUT

set.seed(77841)
## 70% of the sample size
#### Spliting for X input
train_size_X <- floor(0.70 * nrow(eeg_x))
ml_sample_X <- sample(c(1:nrow(eeg_x)), size = train_size_X, replace = FALSE)
train_X <- eeg_x[ml_sample_X, ]
test_X <- eeg_x[-ml_sample_X, ]




#### Spliting for y output
train_size_y <- floor(0.70 * nrow(eeg_y))
ml_sample_y <- sample(c(1:nrow(eeg_y)), size = train_size_y, replace = FALSE)
train_y <- eeg_y[ml_sample_y, ]
test_y <- eeg_y[-ml_sample_y, ]

str(train_X)
str(test_X)

str(train_y)
str(test_y)



```

### Task 2.7.1 Estimate model parameter use training dataset

```{r}
# Estimate model parameter use training dataset
# Using Bayesian inference for parameter estimation
# Prediction
# predict(model_two, data = train, interval = "confidence")
# p(theta|D) = p(theta, D) / p(D) = p(D|theta)p(theta)/p(D)


x1_train = train_X$V1
x2_train = train_X$V2
x3_train = train_X$V3
x4_train = train_X$V4
y_train = train_y

model_three_train = cbind(x2_train, x1_train^3, x3_train^4, 1)

theta_hat_3_train <- solve(t(model_three_train) %*% model_three_train) %*% t(model_three_train) %*% train_y
print("Hat 3")
print(theta_hat_3_train)

```

### Task 2.7.2 Compute the model's output / prediction in 95%


```{r}
# Regression

# y = f(x, theta) + Epsilon

# x is input (independent) variables
# y is output (dependent) variables 
# Theta is t

input_test=as.matrix(test_X)

y_theta = input_test%*%theta_hat_3_train # y
head(y_theta)
```

```{r}
Xtes = as.matrix(test_X)
test_X = Xtes
```

### Task 2.7.3 Compute the 95% (model prediction confidence intervals and plot with error bars and model prediction as well as testing data samples.

```{r}
# compute a CI Confidence interval 

# Var(y_hat) = E((y_hat - y)^2) = E((x(theta_hat - theta))^2) 
# = x*E((theta_hat - theta)*(theta_hat - theta)^transpose)x^Transpose 
# = Sigma^2*x*(X^transpose*X)^-1x^transpose

#E is error rate
#y is original set

number_of_length = nrow(test_X)
print(number_of_length)

X = as.matrix(eeg_x)
sse = norm(error_rate_dis_3 , type = "2")^2
sigma_two = sse/( number_of_length - 1 ) # error variance sigma^2

cov_thetaHat = sigma_two * (solve(t(test_X) %*% test_X))
cov_thetaHat_inv = (t(X) %*% X) * (1/sigma_two) # inverse of cov_thetaHat
det_cov_thetaHat = det(cov_thetaHat) # determinent of cov_thetaHat

y_hat = test_X %*% theta_hat_3_train


###########################################################################


Xtes = as.matrix(test_X)
test_X = Xtes

number_of_parameters = 4 # model three
# # y_hat = X %*% theta_hat_3 # error rate

var_y_hat = matrix(0 , number_of_length , 1)

for( i in 1:number_of_length) {
  X_i = matrix(test_X[i,] , 1 , number_of_parameters ) # X[i,] creates a vector. Convert it to matrix
  var_y_hat[i, ] = X_i %*% cov_thetaHat %*% t(X_i)
}

trans_y_hat = t(y_hat) 

CI = 2 * sqrt(var_y_hat) # Confidence interval
plot(trans_y_hat, test_X)


```

